import argparse
import asyncio
import json
import logging
import os
import time
from typing import List, Optional, Tuple, Dict, Any

from openai import AsyncOpenAI, RateLimitError, APIError
from pydantic import ValidationError

from config import config
from extract_mhtml import extract_text_from_mhtml, read_mhtml_file
from hh_parser import HHParser
from generate_report import generate_markdown_report
from models import CandidateAnalysis

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ LLM –∫–ª–∏–µ–Ω—Ç–∞
client = AsyncOpenAI(api_key=config.LLM_API_KEY, base_url=config.LLM_BASE_URL)

# –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å Rate Limit)
MAX_CONCURRENT_REQUESTS = 5
semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)


async def get_llm_analysis(
    prompt_data: Dict[str, str], prompt_template_name: str = "hr_expert_v2.txt"
) -> Optional[Dict[str, Any]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞.
    """
    max_retries = 5
    base_delay = 5

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è, –Ω–æ –±—ã—Å—Ç—Ä–∞—è)
        prompt_template = config.load_prompt(prompt_template_name)
    except FileNotFoundError:
        logger.error(f"–®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ {prompt_template_name} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return None

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
    final_prompt = prompt_template
    for key, value in prompt_data.items():
        if value:
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö (–Ω–µ JSON), –∏–ª–∏ –¥–ª—è –≤—Å–µ—Ö?
            # JSON –ª—É—á—à–µ –Ω–µ —Ä–µ–∑–∞—Ç—å –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ.
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø–æ–∫–∞ –Ω–µ —Ä–µ–∂–µ–º, –∏–ª–∏ —Ä–µ–∂–µ–º –∞–∫–∫—É—Ä–∞—Ç–Ω–æ.
            # –ï—Å–ª–∏ —ç—Ç–æ JSON, —Ç–æ –æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–ª–∏–Ω–Ω—ã–º.
            final_prompt = final_prompt.replace(f"{{{key}}}", str(value))

    async with semaphore:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
        for attempt in range(max_retries):
            try:
                # logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}...")

                response = await client.chat.completions.create(
                    model=config.LLM_MODEL,
                    temperature=config.LLM_TEMPERATURE,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are a precise JSON-outputting engine. Output ONLY valid JSON matching the schema.",
                        },
                        {"role": "user", "content": final_prompt},
                    ],
                    response_format={"type": "json_object"},
                )

                content = response.choices[0].message.content
                if not content:
                    logger.warning("–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM")
                    return None

                # –û—á–∏—Å—Ç–∫–∞ markdown –±–ª–æ–∫–æ–≤ ```json ... ```
                cleaned_content = _clean_json_content(content)

                try:
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ Pydantic
                    analysis_data = CandidateAnalysis.model_validate_json(
                        cleaned_content
                    )
                    return analysis_data.model_dump()
                except ValidationError as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ Pydantic: {e}")
                    return None
                except json.JSONDecodeError:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {cleaned_content[:100]}...")
                    return None

            except RateLimitError:
                wait_time = base_delay * (2**attempt)
                logger.warning(f"RateLimit (429). –ñ–¥–µ–º {wait_time} —Å–µ–∫...")
                await asyncio.sleep(wait_time)
            except APIError as e:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ 429 –æ—Ç OpenRouter –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
                if getattr(e, "code", None) == 429:
                    wait_time = base_delay * (2**attempt)
                    logger.warning(f"API 429. –ñ–¥–µ–º {wait_time} —Å–µ–∫...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"API Error: {e}")
                    raise e
            except Exception as e:
                logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ API: {e}")
                return None

    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫")
    return None


def _clean_json_content(content: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç markdown –æ–±–µ—Ä—Ç–∫–∏ –∏–∑ JSON —Å—Ç—Ä–æ–∫–∏."""
    content = content.strip()
    if content.startswith("```"):
        content = content.strip("`")
        if content.startswith("json"):
            content = content[4:]
    return content.strip()


def get_candidate_files(work_dir: str) -> Tuple[List[str], List[str]]:
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ —Ä–µ–∑—é–º–µ."""
    if not os.path.exists(work_dir):
        logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {work_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return [], []

    files = [f for f in os.listdir(work_dir) if f.lower().endswith(".mhtml")]
    vacancies = []
    resumes = []

    for f in files:
        full_path = os.path.join(work_dir, f)
        if config.VACANCY_KEYWORD.lower() in f.lower():
            vacancies.append(full_path)
        else:
            resumes.append(full_path)

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ: –í–∞–∫–∞–Ω—Å–∏–π={len(vacancies)}, –†–µ–∑—é–º–µ={len(resumes)}.")
    return vacancies, resumes


async def process_pair(
    resume_path: str, vacancy_path: str, vacancy_data: Any, parser_type: str, prompt_template: str
) -> Optional[Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –ø–∞—Ä—É (–†–µ–∑—é–º–µ, –í–∞–∫–∞–Ω—Å–∏—è)."""
    resume_filename = os.path.basename(resume_path)
    logger.info(f"–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞: {resume_filename}")

    prompt_data = {}

    if parser_type == "new":
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º HHParser (JSON)
        # vacancy_data —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å dict –∏–ª–∏ json string
        content = read_mhtml_file(resume_path)
        if not content:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {resume_filename}")
            return None
            
        parser = HHParser()
        resume_json_obj = parser.parse(content)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        # vacancy_data –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –∫–∞–∫ dict, –µ—Å–ª–∏ parser_type=new
        prompt_data["resume_json"] = json.dumps(resume_json_obj, ensure_ascii=False, indent=2)
        prompt_data["vacancy_json"] = json.dumps(vacancy_data, ensure_ascii=False, indent=2)
        
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Old Parser (Markdown)
        resume_text = extract_text_from_mhtml(resume_path)
        if not resume_text:
             logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {resume_filename}")
             return None
             
        prompt_data["resume_text"] = resume_text
        prompt_data["vacancy_text"] = vacancy_data # vacancy_data –∑–¥–µ—Å—å string

    analysis = await get_llm_analysis(prompt_data, prompt_template_name=prompt_template)

    if analysis:
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        analysis["vacancy_file"] = os.path.basename(vacancy_path)
        analysis["resume_file"] = resume_filename

        score = analysis.get("scoring", {}).get("total_score", "N/A")
        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {resume_filename} (Score: {score})")
        return analysis
    else:
        logger.warning(f"‚ùå –ü—Ä–æ–≤–∞–ª: {resume_filename}")
        return None


async def process_batch_async(
    vacancies: List[str], resumes: List[str], parser_type: str, prompt_template: str
) -> List[Dict[str, Any]]:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π."""
    tasks = []

    for vacancy_path in vacancies:
        logger.info(f"--- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏: {os.path.basename(vacancy_path)} ---")
        
        vacancy_data = None
        if parser_type == "new":
            content = read_mhtml_file(vacancy_path)
            if content:
                parser = HHParser()
                vacancy_data = parser.parse(content)
        else:
            vacancy_data = extract_text_from_mhtml(vacancy_path)

        if not vacancy_data:
            logger.error(f"–ü—Ä–æ–ø—É—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_path} (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)")
            continue

        for resume_path in resumes:
            task = asyncio.create_task(
                process_pair(resume_path, vacancy_path, vacancy_data, parser_type, prompt_template)
            )
            tasks.append(task)

    logger.info(f"–ó–∞–ø—É—Å–∫ {len(tasks)} –∑–∞–¥–∞—á –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ...")
    results = await asyncio.gather(*tasks)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—É–¥–∞–ª—è–µ–º None)
    valid_results = [r for r in results if r is not None]
    return valid_results


def save_results(results: List[Dict[str, Any]], reports_dir: str = "reports") -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç."""
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(reports_dir, f"analysis_results_{timestamp}.json")

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    logger.info(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
    report_content = generate_markdown_report(results)
    if report_content:
        report_filename = os.path.join(reports_dir, f"report_{timestamp}.md")
        with open(report_filename, "w", encoding="utf-8") as f:
            f.write(report_content)
        logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_filename}")


async def async_main():
    parser = argparse.ArgumentParser(description="AI Recruitment Assistant")
    parser.add_argument("--parser", choices=["old", "new"], default="new", help="Type of parser to use (old=text, new=json)")
    parser.add_argument("--prompt", default=None, help="Prompt template filename (defaults to hr_expert_json.txt for new, hr_expert_legacy_markdown.txt for old)")
    
    args = parser.parse_args()
    
    # Auto-select prompt if not provided
    if args.prompt is None:
        if args.parser == "new":
            args.prompt = "hr_expert_json.txt"
        else:
            args.prompt = "hr_expert_legacy_markdown.txt"
    
    work_dir = "resume vs vacancy"
    vacancies, resumes = get_candidate_files(work_dir)

    if not vacancies or not resumes:
        logger.warning("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    logger.info(f"–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞. –ü–∞—Ä—Å–µ—Ä: {args.parser}, –ü—Ä–æ–º–ø—Ç: {args.prompt}")
    start_time = time.time()

    results = await process_batch_async(vacancies, resumes, args.parser, args.prompt)

    duration = time.time() - start_time
    logger.info(f"\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration:.2f} —Å–µ–∫. ===")
    logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)}")

    if results:
        save_results(results)


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —á–µ—Ä–µ–∑ 'python analyze_candidates.py'"""
    asyncio.run(async_main())


if __name__ == "__main__":
    main()
